"""
Sarcasm Detection - Model Evaluation & Error Analysis
=====================================================
Author: Nikhil Guna
Course: EECS 543 - AI Ethics
Assignment 2

This script evaluates the AI model's performance on the study dataset
and generates the error analysis report.

Usage:
    python evaluate_model.py

Requirements:
    - model_outputs.json (generated by run_inference.py)
    - No additional packages needed (uses only Python stdlib)
"""

import json
from collections import Counter

def load_results(filepath="model_outputs.json"):
    """Load model outputs from JSON file."""
    with open(filepath) as f:
        return json.load(f)

def compute_metrics(results):
    """Compute accuracy, precision, recall, and F1 for SARCASM class."""
    tp = sum(1 for r in results if r['ground_truth'] == 'SARCASM' and r['model_prediction'] == 'SARCASM')
    tn = sum(1 for r in results if r['ground_truth'] == 'NOT_SARCASM' and r['model_prediction'] == 'NOT_SARCASM')
    fp = sum(1 for r in results if r['ground_truth'] == 'NOT_SARCASM' and r['model_prediction'] == 'SARCASM')
    fn = sum(1 for r in results if r['ground_truth'] == 'SARCASM' and r['model_prediction'] == 'NOT_SARCASM')
    
    accuracy = (tp + tn) / len(results)
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    
    return {
        'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn,
        'accuracy': accuracy, 'precision': precision,
        'recall': recall, 'f1': f1
    }

def confidence_analysis(results):
    """Analyze model confidence for correct vs incorrect predictions."""
    correct = [r for r in results if r['model_prediction'] == r['ground_truth']]
    incorrect = [r for r in results if r['model_prediction'] != r['ground_truth']]
    
    avg_correct = sum(r['model_confidence'] for r in correct) / len(correct)
    avg_incorrect = sum(r['model_confidence'] for r in incorrect) / len(incorrect)
    
    return avg_correct, avg_incorrect

def categorize_errors(results):
    """Categorize model errors into patterns based on reasoning and content."""
    errors = [r for r in results if r['model_prediction'] != r['ground_truth']]
    fp = [r for r in errors if r['ground_truth'] == 'NOT_SARCASM']
    fn = [r for r in errors if r['ground_truth'] == 'SARCASM']
    
    # Categorize false positives
    fp_categories = {
        'Hyperbole/exaggeration in genuine statement': 0,
        'Humor/jokes mistaken for sarcasm': 0,
        'Rhetorical/skeptical tone mistaken for sarcasm': 0,
        'Edgy/provocative genuine opinion': 0,
        'Cultural reference misread': 0,
    }
    
    for r in fp:
        reasoning = r['model_reasoning'].lower()
        if any(w in reasoning for w in ['exaggerat', 'absurd', 'hyperbol', 'over-the-top']):
            fp_categories['Hyperbole/exaggeration in genuine statement'] += 1
        elif any(w in reasoning for w in ['humor', 'joke', 'funny', 'playful', 'lighthearted']):
            fp_categories['Humor/jokes mistaken for sarcasm'] += 1
        elif any(w in reasoning for w in ['rhetorical', 'question', 'tone', 'ironic tone']):
            fp_categories['Rhetorical/skeptical tone mistaken for sarcasm'] += 1
        elif any(w in reasoning for w in ['mock', 'ridicul', 'dismiss', 'feign']):
            fp_categories['Edgy/provocative genuine opinion'] += 1
        else:
            fp_categories['Cultural reference misread'] += 1
    
    # Categorize false negatives
    fn_categories = {
        'Deadpan/subtle sarcasm without obvious markers': 0,
        'Sarcasm via genuine-sounding continuation': 0,
        'Context-dependent (requires thread knowledge)': 0,
    }
    
    for r in fn:
        reasoning = r['model_reasoning'].lower()
        if any(w in reasoning for w in ['genuine', 'straightforward', 'sincere', 'literal']):
            fn_categories['Deadpan/subtle sarcasm without obvious markers'] += 1
        elif any(w in reasoning for w in ['context', 'thread', 'previous']):
            fn_categories['Context-dependent (requires thread knowledge)'] += 1
        else:
            fn_categories['Sarcasm via genuine-sounding continuation'] += 1
    
    return fp_categories, fn_categories, len(errors), len(fp), len(fn)

def main():
    results = load_results()
    print(f"Loaded {len(results)} results")
    print(f"Label distribution: {dict(Counter(r['ground_truth'] for r in results))}")
    
    # Compute metrics
    m = compute_metrics(results)
    print(f"\n{'='*60}")
    print("EVALUATION METRICS")
    print(f"{'='*60}")
    print(f"\nConfusion Matrix:")
    print(f"{'':>25} Pred SARCASM  Pred NOT_SARCASM")
    print(f"  Actual SARCASM       {m['tp']:>8}        {m['fn']:>8}")
    print(f"  Actual NOT_SARCASM   {m['fp']:>8}        {m['tn']:>8}")
    print(f"\nAccuracy:  {m['accuracy']:.4f} ({m['tp']+m['tn']}/{len(results)})")
    print(f"Precision: {m['precision']:.4f}")
    print(f"Recall:    {m['recall']:.4f}")
    print(f"F1 Score:  {m['f1']:.4f}")
    
    # Confidence analysis
    avg_correct, avg_incorrect = confidence_analysis(results)
    print(f"\nConfidence Analysis:")
    print(f"  Avg confidence (correct predictions):   {avg_correct:.1f}%")
    print(f"  Avg confidence (incorrect predictions): {avg_incorrect:.1f}%")
    
    # Error analysis
    fp_cats, fn_cats, total_err, num_fp, num_fn = categorize_errors(results)
    print(f"\n{'='*60}")
    print("ERROR DISTRIBUTION TABLE")
    print(f"{'='*60}")
    print(f"Total errors: {total_err}")
    print(f"\nFALSE POSITIVES ({num_fp} errors, {num_fp/total_err*100:.1f}% of total)")
    for cat, count in sorted(fp_cats.items(), key=lambda x: -x[1]):
        print(f"  {cat:<50} {count:>3} ({count/total_err*100:.1f}%)")
    print(f"\nFALSE NEGATIVES ({num_fn} errors, {num_fn/total_err*100:.1f}% of total)")
    for cat, count in sorted(fn_cats.items(), key=lambda x: -x[1]):
        print(f"  {cat:<50} {count:>3} ({count/total_err*100:.1f}%)")

if __name__ == '__main__':
    main()
